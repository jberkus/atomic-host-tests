---
# vim: set ft=ansible:
#
# !!!NOTE!!! This playbook was tested using Ansible 2.1; it is recommended
# that the same version is used.
#
# This playbook is actually multiple playbooks contained in a single file.
# I had to take this approach, because I encountered difficulties re-using
# the same roles with non-unique variables.  Sometimes the roles would be
# executed with the use of the 'always' tag, sometimes not.
#
# The wrinkle in this approach is that sharing global variables across the
# multiple playbooks requires the use of a file containing the variables
# which is included in each playbook with the 'vars_files' directive.
#
# The sanity tests performed in this set of playbooks:
#   - check permissions on /tmp are correct (RHBZ 1276775) after each reboot
#   - add user to system, verify it is present after upgrade
#   - make changes to /etc, verify changes are present after upgrade
#   - add directory/file to /var, verify they persist after upgrade/rollback
#   - add user post-upgrade, verify it is not present after rollback
#   - add directory/file to /var post-upgrade, verify they persist after
#     rollback
#
# The intent is to expand the sanity test coverage in this set of playbooks
# with small, focused tests going forward.  Additional functional tests that
# are more expansive should be handled in separate playbooks.
#
- name: Improved Sanity Test - Pre-Upgrade
  hosts: all
  become: yes

  vars_files:
    - vars.yml

  roles:
    # This playbook requires Ansible 2.1 and an Atomic Host
    - { role: ansible_version_check, avc_major: "2", avc_minor: "1", tags: ['ansible_version_check'] }

    - role: atomic_host_check
      tags:
        - atomic_host_check

    - role: selinux_verify
      tags:
        - selinux_verify

    # Subscribe if the system is RHEL
    - role: redhat_subscription
      when: ansible_distribution == 'RedHat'
      tags:
        - redhat_subscription

    # Check that /tmp is properly setup
    - role: tmp_check_perms
      tags:
        - tmp_check_perms

    # Check that the RPM database is functional
    - role: rpmdb_verify
      tags:
        - rpmdb_verify

    # Add users, make changes to /etc, and add things to /var before the
    # the system is upgraded
    - { role: user_add, ua_uid: "{{ g_uid1 }}", tags: ['user_add'] }

    - { role: user_verify_present, uvp_uid: "{{ g_uid1 }}", tags: ['user_verify_present'] }

    - role: etc_modify
      tags:
        - etc_modify

    - role: etc_verify_changes
      tags:
        - etc_verify_changes

    - { role: var_add_files, vaf_filename: "{{ g_file1 }}", vaf_dirname: "{{ g_dir1 }}", tags: ['var_add_files'] }

    - { role: var_files_present, vfp_filename: "{{ g_file1 }}", vfp_dirname: "{{ g_dir1 }}", tags: ['var_files_present'] }

    # Pull down docker base images, build a layered, image, run it, then
    # remove the container.  But we keep the image around to run later.
    # Run atomic test prior to running docker test so the built images can be run later.
    - role: osname_set_fact
      tags:
        - osname_set_fact

    - role: atomic_host_status_verify
      tags:
        - atomic_host_status_verify

    - role: atomic_pull_verify
      tags:
        - atomic_pull_verify

    - role: docker_remove_all
      tags:
        - docker_remove_all

    - role: atomic_installation_verify
      tags:
        - atomic_installation_verify

    - role: docker_remove_all
      tags:
        - docker_remove_all

    - role: atomic_scan_verify
      tags:
        - atomic_scan_verify

    - role: docker_remove_all
      tags:
        - docker_remove_all

    - role: docker_pull_base_image
      tags:
        - docker_pull_base_image

    - role: docker_build_httpd
      tags:
        - docker_build_httpd

    - { role: atomic_run_verify, container: "{{ g_osname }}_httpd", tags: ['atomic_run_verify'] }

    - { role: atomic_stop_verify, container: "{{ g_osname }}_httpd", tags: ['atomic_stop_verify'] }

    - role: docker_rm_httpd_container
      tags:
        - docker_rm_httpd_container

    - role: docker_run_httpd
      tags:
        - docker_run_httpd

    - role: docker_rm_httpd_container
      tags:
        - docker_rm_httpd_container

    # Install, run and uninstall cockpit using atomic command
    - role: atomic_installation_verify
      tags:
        - atomic_installation_verify

    # Upgrade and reboot
    - role: rpm_ostree_upgrade
      tags:
        - rpm_ostree_upgrade

    - role: reboot
      tags:
        - reboot

- name: Improved Sanity Test - Post-Upgrade
  hosts: all
  become: yes

  vars_files:
    - vars.yml

  roles:
    # This playbook requires Ansible 2.1 and an Atomic Host
    - { role: ansible_version_check, avc_major: "2", avc_minor: "0", tags: ['ansible_version_check'] }

    - role: atomic_host_check
      tags:
        - atomic_host_check

    - role: selinux_verify
      tags:
        - selinux_verify

    # Check that /tmp is properly setup
    - role: tmp_check_perms
      tags:
        - tmp_check_perms

    # Check that the RPM database is functional
    - role: rpmdb_verify
      tags:
        - rpmdb_verify

    # Verify that the new users, /etc changes, and /var additions are still
    # present after the upgrade
    - { role: user_verify_present, uvp_uid: "{{ g_uid1 }}", tags: ['user_verify_present'] }

    - role: etc_verify_changes
      tags:
        - etc_verify_changes

    - { role: var_files_present, vfp_filename: "{{ g_file1 }}", vfp_dirname: "{{ g_dir1 }}", tags: ['var_files_present'] }

    # Add more users and more files to /var ahead of the rollback
    - { role: user_add, ua_uid: "{{ g_uid2 }}", tags: ['user_add'] }

    - { role: user_verify_present, uvp_uid: "{{ g_uid2 }}", tags: ['user_verify_present'] }

    - { role: var_add_files, vaf_filename: "{{ g_file2 }}", vaf_dirname: "{{ g_dir2 }}", tags: ['var_add_files'] }

    - { role: var_files_present, vfp_filename: "{{ g_file2 }}", vfp_dirname: "{{ g_dir2 }}", tags: ['var_files_present'] }

    # Install, run and uninstall cockpit using atomic command
    - role: atomic_installation_verify
      tags:
        - atomic_installation_verify

    # Run the previously created docker layered image, then remove the container and the image.
    - role: osname_set_fact
      tags:
        - osname_set_fact

    - { role: atomic_run_verify, container: "{{ g_osname }}_httpd", tags: ['atomic_run_verify'] }

    - { role: atomic_stop_verify, container: "{{ g_osname }}_httpd", tags: ['atomic_stop_verify'] }

    - role: docker_rm_httpd_container
      tags:
        - docker_rm_httpd_container

    - role: docker_run_httpd
      tags:
        - docker_run_httpd

    - role: docker_rm_httpd_container
      tags:
        - rm_httpd_container

    - role: docker_rmi_httpd_image
      tags:
        - docker_rmi_httpd_image

    # Re-run atomic tests
    - role: atomic_host_status_verify
      tags:
        - atomic_host_status_verify

    - role: docker_remove_all
      tags:
        - docker_remove_all

    - role: atomic_pull_verify
      tags:
        - atomic_pull_verify

    - role: docker_remove_all
      tags:
        - docker_remove_all

    - role: atomic_installation_verify
      tags:
        - atomic_installation_verify

    - role: docker_remove_all
      tags:
        - docker_remove_all

    - role: atomic_scan_verify
      tags:
        - atomic_scan_verify

    # Rollback and reboot
    - role: rpm_ostree_rollback
      tags:
        - rpm_ostree_rollback

    - role: reboot
      tags:
        - reboot

- name: Improved Sanity Test - Post-Rollback
  hosts: all
  become: yes

  vars_files:
    - vars.yml

  roles:
    # This playbook requires Ansible 2.1 and an Atomic Host
    - { role: ansible_version_check, avc_major: "2", avc_minor: "1", tags: ['ansible_version_check'] }

    - role: atomic_host_check
      tags:
        - atomic_host_check

    - role: selinux_verify
      tags:
        - selinux_verify

    # Check that /tmp is properly setup
    - role: tmp_check_perms
      tags:
        - tmp_check_perms

    # Check that the RPM database is functional
    - role: rpmdb_verify
      tags:
        - rpmdb_verify

    # Verify changes still exist in original tree
    - role: etc_verify_changes
      tags:
        - etc_verify_changes

    # Verify first user added is still present in original tree
    - { role: user_verify_present, uvp_uid: "{{ g_uid1 }}", tags: ['user_verify_present'] }

    # Verify first files dropped in /var are still in original tree
    - { role: var_files_present, vfp_filename: "{{ g_file1 }}", vfp_dirname: "{{ g_dir1 }}", tags: ['var_files_present'] }

    # Verify that the newest user is not present in the previous tree
    - { role: user_verify_missing, uvm_uid: "{{ g_uid2 }}", tags: ['user_verify_missing'] }

    # Verify the most recent changes to /var are present
    - { role: var_files_present, vfp_filename: "{{ g_file2 }}", vfp_dirname: "{{ g_dir2 }}", tags: ['var_files_present'] }
