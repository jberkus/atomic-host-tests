---
# vim: set ft=ansible:
#
# !!!NOTE!!! This playbook was tested using Ansible 2.2; it is recommended
# that the same version is used.
#
# This playbook is actually multiple playbooks contained in a single file.
# I had to take this approach, because I encountered difficulties re-using
# the same roles with non-unique variables.  Sometimes the roles would be
# executed with the use of the 'always' tag, sometimes not.
#
# The wrinkle in this approach is that sharing global variables across the
# multiple playbooks requires the use of a file containing the variables
# which is included in each playbook with the 'vars_files' directive.
#
# The sanity tests performed in this set of playbooks:
#   - check permissions on /tmp are correct (RHBZ 1276775) after each reboot
#   - add user to system, verify it is present after upgrade
#   - make changes to /etc, verify changes are present after upgrade
#   - add directory/file to /var, verify they persist after upgrade/rollback
#   - add user post-upgrade, verify it is not present after rollback
#   - add directory/file to /var post-upgrade, verify they persist after
#     rollback
#
# The intent is to expand the sanity test coverage in this set of playbooks
# with small, focused tests going forward.  Additional functional tests that
# are more expansive should be handled in separate playbooks.
#
- name: Improved Sanity Test - Pre-Upgrade
  hosts: all
  become: yes

  tags:
    - pre_upgrade

  vars_files:
    - vars.yml

  roles:
    # This playbook requires Ansible 2.2 and an Atomic Host
    - role: ansible_version_check
      avc_major: "2"
      avc_minor: "2"
      tags:
        - ansible_version_check
        - cloud_image

    - role: atomic_host_check
      tags:
        - atomic_host_check
        - cloud_image

    # Set some facts that are used in multiple roles
    - role: osname_set_fact
      tags:
        - osname_set_fact
        - cloud_image

    # Verify that rpm-ostree can be run as non-root or sudo
    - role: command_privilege_verify
      cmd: rpm-ostree status
      user: bin
      expect_failure: false
      tags:
        - command_privilege_verify
        - cloud_image

    # Verify that the output from 'atomic host status' matches 'rpm-ostree status'
    - role: atomic_host_status_verify
      tags:
        - atomic_host_status_verify
        - cloud_image

    # Verify SELinux labels are correct
    - role: selinux_verify
      tags:
        - selinux_verify
        - cloud_image

    # Verify that SELinux file contexts can be modified
    - role: semanage_fcontext_mod
      test_dir_match: "{{ g_semanage_test_dir_match }}"
      test_context: "{{ g_semanage_test_context }}"
      tags:
        - semanage_fcontext_mod
        - cloud_image

    - name: semanage_fcontext_verify
      test_dir: "{{ g_semanage_test_dir }}"
      test_context: "{{ g_semanage_test_context }}"
      tags:
        - semanage_fcontext_verify
        - cloud_image

    - role: selinux_boolean
      tags:
        - selinux_boolean
        - cloud_image

    # Subscribe if the system is RHEL
    - role: redhat_subscription
      when: ansible_distribution == 'RedHat'
      tags:
        - redhat_subscription
        - cloud_image

    # Verify that 'docker pull' is successful. (Note: this is a different
    # operation than 'atomic pull')
    - role: docker_pull_run_remove
      tags:
        - docker_pull_run_remove
        - cloud_image

    # Validate 'atomic pull', 'atomic scan' works correctly.
    # Remove images after test of each command.
    # Do this ahead of 'ostree admin unlock' because of overlay + SELinux
    # incompatibility on RHEL. (This should be fixed in 7.4)
    - role: atomic_pull_verify
      tags:
        - atomic_pull_verify
        - cloud_image

    - role: docker_remove_all
      tags:
        - docker_remove_all
        - cloud_image

    - role: atomic_scan_verify
      tags:
        - atomic_scan_verify
        - cloud_image

    - role: docker_remove_all
      tags:
        - docker_remove_all
        - cloud_image

    # Install package using package layering
    - role: rpm_ostree_install
      packages: "{{ g_pkg }}"
      reboot: false
      tags:
        - rpm_ostree_install

    - role: reboot
      wait_for_services:
        - docker
      tags:
        - reboot_pre_up_install
        - cloud_image

    - role: rpm_ostree_install_verify
      package: "{{ g_pkg }}"
      tags:
        - rpm_ostree_install_verify

    # Verify admin unlock by installing an RPM
    - role: ostree_admin_unlock_hotfix
      tags:
        - ostree_admin_unlock_hotfix
        - cloud_image

    - role: overlayfs_verify_present
      tags:
        - overlayfs_verify_present
        - cloud_image

    - role: rpm_install
      rpm_name: "{{ g_rpm_name }}"
      tags:
        - rpm_install
        - cloud_image

    - role: package_verify_present
      rpm_name: "{{ g_rpm_name }}"
      tags:
        - package_verify_present
        - cloud_image

    # Check that /tmp has the proper permissions
    - role: tmp_check_perms
      tags:
        - tmp_check_perms
        - cloud_image

    # Check that the RPM database is functional
    - role: rpmdb_verify
      tags:
        - rpmdb_verify
        - cloud_image

    # Add users, make changes to /etc, and add things to /var before the
    # the system is upgraded
    - role: user_add
      ua_uid: "{{ g_uid1 }}"
      tags:
        - user_add
        - cloud_image

    - role: user_verify_present
      uvp_uid: "{{ g_uid1 }}"
      tags:
        - user_verify_present
        - cloud_image

    - role: etc_modify
      tags:
        - etc_modify
        - cloud_image

    - role: etc_verify_changes
      tags:
        - etc_verify_changes
        - cloud_image

    - role: var_add_files
      vaf_filename: "{{ g_file1 }}"
      vaf_dirname: "{{ g_dir1 }}"
      tags:
        - var_add_files
        - cloud_image

    - role: var_files_present
      vfp_filename: "{{ g_file1 }}"
      vfp_dirname: "{{ g_dir1 }}"
      tags:
        - var_files_present
        - cloud_image

    # Pull down docker base images, build a layered image, run it, then
    # remove the container.  But we keep the image around to run later.
    # Run 'atomic run/stop' tests prior to running 'docker run' test so
    # the built images can be run later.
    - role: docker_pull_base_image
      tags:
        - docker_pull_base_image
        - cloud_image

    - role: docker_build_httpd
      tags:
        - docker_build_httpd
        - cloud_image

    - role: atomic_run_verify
      container: "{{ g_httpd_name }}"
      tags:
        - atomic_run_verify
        - cloud_image

    - role: atomic_stop_verify
      container: "{{ g_httpd_name }}"
      tags:
        - atomic_stop_verify
        - cloud_image

    - role: docker_rm_httpd_container
      tags:
        - docker_rm_httpd_container
        - cloud_image

    - role: docker_run_httpd
      tags:
        - docker_run_httpd
        - cloud_image

    - role: docker_rm_httpd_container
      tags:
        - docker_rm_httpd_container
        - cloud_image

    # Install, run and uninstall cockpit using atomic command
    - role: atomic_installation_verify
      tags:
        - atomic_installation_verify
        - cloud_image

    # Upgrade and reboot
    - role: rpm_ostree_upgrade
      tags:
        - rpm_ostree_upgrade

    - role: reboot
      wait_for_services:
        - docker
      tags:
        - reboot_pre_upgrade

- name: Improved Sanity Test - Post-Upgrade
  hosts: all
  become: yes

  tags:
    - post_upgrade

  vars_files:
    - vars.yml

  roles:
    # This playbook requires Ansible 2.2 and an Atomic Host
    - role: ansible_version_check
      avc_major: "2"
      avc_minor: "2"
      tags:
        - ansible_version_check
        - cloud_image

    - role: atomic_host_check
      tags:
        - atomic_host_check

    # Setup facts again. (Need to use the 'check_mode: no' option to ensure the
    # role runs again)
    - role: osname_set_fact
      tags:
        - osname_set_fact
        - cloud_image
      check_mode: no

    # We remove any subscriptions after the upgrade to verify that
    # 'rpm-ostree status' with the 'unconfigured-state' field present.
    # https://bugzilla.redhat.com/show_bug.cgi?id=1421867
    - role: redhat_unsubscribe
      unconfigured_state: true
      when: ansible_distribution == 'RedHat'
      tags:
        - redhat_unsubscribe

    # Verify that rpm-ostree can be run as non-root or sudo
    - role: command_privilege_verify
      cmd: rpm-ostree status
      user: bin
      expect_failure: false
      tags:
        - command_privilege_verify

    # Compare 'atomic host status' and 'rpm-ostree status' again
    - role: atomic_host_status_verify
      stop_daemon: true
      tags:
        - atomic_host_status_verify

    # Re-subscribe
    - role: redhat_subscription
      when: ansible_distribution == 'RedHat'
      tags:
        - redhat_subscription

    # Verify correct SELinux labels again
    - role: selinux_verify
      tags:
        - selinux_verify

    # Verify the change to the SELinux file context persisted
    - role: semanage_fcontext_verify
      test_dir: "{{ g_semanage_test_dir }}"
      test_context: "{{ g_semanage_test_context }}"
      tags:
        - semanage_fcontext_verify

    # Verify the SELinux boolean changes
    - role: selinux_boolean_verify
      tags:
        - selinux_boolean_verify
        - cloud_image

    - role: docker_pull_run_remove
      tags:
        - docker_pull_run_remove

    # Check layered package is still installed
    - role: rpm_ostree_install_verify
      package: "{{ g_pkg }}"
      tags:
        - rpm_ostree_install_verify

    - role: rpm_ostree_install
      packages: httpd
      reboot: false
      tags:
        - rpm_ostree_install
        - cloud_image

    - role: reboot
      tags:
        - reboot_post_up_install
        - cloud_image

    - role: rpm_ostree_install_verify
      package: "{{ g_pkg }}"
      tags:
        - rpm_ostree_install_verify

    - role: rpm_ostree_install_verify
      package: httpd
      tags:
        - rpm_ostree_install_verify
        - cloud_image
      check_mode: no

    - role: httpd_start
      httpd_port: 8080
      tags:
        - httpd_start
        - cloud_image

    - role: url_verify
      url: http://localhost:8080
      destination: /dev/null
      tags:
        - url_verify
        - cloud_image

    # Check admin unlock overlayfs is no longer there after upgrade
    - role: overlayfs_verify_missing
      tags:
        - overlayfs_verify_missing

    # Verify admin unlocked package is removed after upgrade
    - role: package_verify_missing
      rpm_name: "{{ g_rpm_name }}"
      tags:
        - package_verify_missing

    # Check that /tmp is properly setup again
    - role: tmp_check_perms
      tags:
        - tmp_check_perms

    # Check that the RPM database is functional again
    - role: rpmdb_verify
      tags:
        - rpmdb_verify

    # Verify that the new users, /etc changes, and /var additions are still
    # present after the upgrade
    - role: user_verify_present
      uvp_uid: "{{ g_uid1 }}"
      tags:
        - user_verify_present
        - cloud_image

    - role: etc_verify_changes
      tags:
        - etc_verify_changes
        - cloud_image

    - role: var_files_present
      vfp_filename: "{{ g_file1 }}"
      vfp_dirname: "{{ g_dir1 }}"
      tags:
        - var_files_present
        - cloud_image

    # Add more users and more files to /var ahead of the rollback
    - role: user_add
      ua_uid: "{{ g_uid2 }}"
      tags:
        - user_add

    - role: user_verify_present
      uvp_uid: "{{ g_uid2 }}"
      tags:
        - user_verify_present

    - role: var_add_files
      vaf_filename: "{{ g_file2 }}"
      vaf_dirname: "{{ g_dir2 }}"
      tags:
        - var_add_files

    - role: var_files_present
      vfp_filename: "{{ g_file2 }}"
      vfp_dirname: "{{ g_dir2 }}"
      tags:
        - var_files_present

    # Install, run and uninstall cockpit using atomic command
    - role: atomic_installation_verify
      tags:
        - atomic_installation_verify

    # Run the previously created docker layered image, then remove the container and the image.
    - role: atomic_run_verify
      container: "{{ g_httpd_name }}"
      tags:
        - atomic_run_verify

    - role: atomic_stop_verify
      container: "{{ g_httpd_name }}"
      tags:
        - atomic_stop_verify

    - role: docker_rm_httpd_container
      tags:
        - docker_rm_httpd_container

    - role: docker_rmi_httpd_image
      tags:
        - docker_rmi_httpd_image

    - role: docker_remove_all
      tags:
        - docker_remove_all
        - cloud_image

    # Validate 'atomic' commands again.
    - role: atomic_pull_verify
      tags:
        - atomic_pull_verify

    - role: docker_remove_all
      tags:
        - docker_remove_all

    - role: atomic_scan_verify
      tags:
        - atomic_scan_verify

    # Rollback and reboot
    - role: rpm_ostree_rollback
      tags:
        - rpm_ostree_rollback

    - role: reboot
      wait_for_services:
        - docker
      tags:
        - reboot_post_upgrade

- name: Improved Sanity Test - Post-Rollback
  hosts: all
  become: yes

  tags:
    - post_rollback

  vars_files:
    - vars.yml

  roles:
    # This playbook requires Ansible 2.2 and an Atomic Host
    - role: ansible_version_check
      avc_major: "2"
      avc_minor: "2"
      tags:
        - ansible_version_check
        - cloud_image

    - role: atomic_host_check
      tags:
        - atomic_host_check

    - role: selinux_verify
      tags:
        - selinux_verify

    # Verify that the change to the SELinux file context still exists in
    # the original deployment
    - role: semanage_fcontext_verify
      test_dir: "{{ g_semanage_test_dir }}"
      test_context: "{{ g_semanage_test_context }}"
      tags:
        - semanage_fcontext_verify

    # Check layered package is still installed
    - role: rpm_ostree_install_verify
      package: "{{ g_pkg }}"
      tags:
        - rpm_ostree_install_verify

    # Check http package is not installed
    - role: rpm_ostree_uninstall_verify
      package: httpd
      tags:
        - rpm_ostree_uninstall_verify

    # Check admin unlock overlayfs missing
    - role: overlayfs_verify_missing
      tags:
        - overlayfs_verify_missing

    # Check that /tmp is properly setup yet again
    - role: tmp_check_perms
      tags:
        - tmp_check_perms

    # Check that the RPM database is functional yet again
    - role: rpmdb_verify
      tags:
        - rpmdb_verify

    # Verify changes still exist in original tree
    - role: etc_verify_changes
      tags:
        - etc_verify_changes

    # Verify first user added is still present in original tree
    - role: user_verify_present
      uvp_uid: "{{ g_uid1 }}"
      tags:
        - user_verify_present

    # Verify first files dropped in /var are still in original tree
    - role: var_files_present
      vfp_filename: "{{ g_file1 }}"
      vfp_dirname: "{{ g_dir1 }}"
      tags:
        - var_files_present

    # Verify that the newest user is not present in the previous tree
    - role: user_verify_missing
      uvm_uid: "{{ g_uid2 }}"
      tags:
        - user_verify_missing

    # Verify the most recent changes to /var are present
    - role: var_files_present
      vfp_filename: "{{ g_file2 }}"
      vfp_dirname: "{{ g_dir2 }}"
      tags:
        - var_files_present

    # cleanup our subscriptions because we are nice people
    - role: redhat_unsubscribe
      when: ansible_distribution == 'RedHat'
      tags:
        - redhat_unsubscribe
        - cloud_image
